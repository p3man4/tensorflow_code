{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train= mnist.train.images\n",
    "Y_train = mnist.train.labels\n",
    "X_test = mnist.test.images\n",
    "y_test = mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_X,batch_Y = mnist.train.next_batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hello = tf.constant('Hello Tensorflow!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Tensorflow!\n"
     ]
    }
   ],
   "source": [
    "sess=tf.Session()\n",
    "print sess.run(hello)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a: 2 b: 3\n",
      "Addition with constants: 5\n",
      "Multiplication with constants: 6\n"
     ]
    }
   ],
   "source": [
    "a =tf.constant(2)\n",
    "b= tf.constant(3)\n",
    "with tf.Session() as sess:\n",
    "    print \"a: %i\" % sess.run(a), \"b: %i\" % sess.run(b)\n",
    "    print \"Addition with constants: %i\" % sess.run(a+b)\n",
    "    print \"Multiplication with constants: %i\" % sess.run(a*b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = tf.placeholder(tf.int16)\n",
    "b = tf.placeholder(tf.int16)\n",
    "add = tf.add(a,b)\n",
    "mul = tf.multiply(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition with variables 5\n",
      "Multiplication with variables 6\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    print \"Addition with variables %i\" % sess.run(add, feed_dict={a:2,b:3})\n",
    "    print \"Multiplication with variables %i\" % sess.run(mul, feed_dict={a:2,b:3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix1 = tf.constant([[3., 3.]])\n",
    "matrix2 = tf.constant([[2.],[2.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "product = tf.matmul(matrix1, matrix2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 12.]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    result = sess.run(product)\n",
    "    print result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xtr,Ytr = mnist.train.next_batch(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Xte,Yte = mnist.test.next_batch(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xtr = tf.placeholder(\"float\",[None,784])\n",
    "xte = tf.placeholder(\"float\",[784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "distance = tf.reduce_sum(tf.abs(tf.add(xtr,tf.negative(xte))), reduction_indices=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = tf.arg_min(distance, 0)\n",
    "accuracy = 0.\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x=tf.constant([[b1,1,1],[1,1,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 784)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1316\n"
     ]
    }
   ],
   "source": [
    "a =tf.constant(3)\n",
    "with tf.Session() as sess:\n",
    "    print sess.run(pred,feed_dict={xtr:Xtr,xte:Xte[2,:]})\n",
    "    #print sess.run(len(Xte))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 0 Prediction: 0 True class: 7\n",
      "Test 1 Prediction: 0 True class: 2\n",
      "Test 2 Prediction: 0 True class: 1\n",
      "Test 3 Prediction: 1 True class: 0\n",
      "Test 4 Prediction: 0 True class: 4\n",
      "Test 5 Prediction: 0 True class: 1\n",
      "Test 6 Prediction: 0 True class: 4\n",
      "Test 7 Prediction: 0 True class: 9\n",
      "Test 8 Prediction: 0 True class: 5\n",
      "Test 9 Prediction: 0 True class: 9\n",
      "Test 10 Prediction: 1 True class: 0\n",
      "Test 11 Prediction: 1 True class: 6\n",
      "Test 12 Prediction: 0 True class: 9\n",
      "Test 13 Prediction: 1 True class: 0\n",
      "Test 14 Prediction: 0 True class: 1\n",
      "Test 15 Prediction: 0 True class: 5\n",
      "Test 16 Prediction: 0 True class: 9\n",
      "Test 17 Prediction: 0 True class: 7\n",
      "Test 18 Prediction: 0 True class: 3\n",
      "Test 19 Prediction: 0 True class: 4\n",
      "Test 20 Prediction: 0 True class: 9\n",
      "Test 21 Prediction: 0 True class: 6\n",
      "Test 22 Prediction: 0 True class: 6\n",
      "Test 23 Prediction: 0 True class: 5\n",
      "Test 24 Prediction: 0 True class: 4\n",
      "Test 25 Prediction: 1 True class: 0\n",
      "Test 26 Prediction: 0 True class: 7\n",
      "Test 27 Prediction: 0 True class: 4\n",
      "Test 28 Prediction: 1 True class: 0\n",
      "Test 29 Prediction: 0 True class: 1\n",
      "Test 30 Prediction: 0 True class: 3\n",
      "Test 31 Prediction: 0 True class: 1\n",
      "Test 32 Prediction: 0 True class: 3\n",
      "Test 33 Prediction: 0 True class: 4\n",
      "Test 34 Prediction: 0 True class: 7\n",
      "Test 35 Prediction: 0 True class: 2\n",
      "Test 36 Prediction: 0 True class: 7\n",
      "Test 37 Prediction: 0 True class: 1\n",
      "Test 38 Prediction: 0 True class: 2\n",
      "Test 39 Prediction: 0 True class: 1\n",
      "Test 40 Prediction: 0 True class: 1\n",
      "Test 41 Prediction: 0 True class: 7\n",
      "Test 42 Prediction: 0 True class: 4\n",
      "Test 43 Prediction: 0 True class: 2\n",
      "Test 44 Prediction: 0 True class: 3\n",
      "Test 45 Prediction: 0 True class: 5\n",
      "Test 46 Prediction: 0 True class: 1\n",
      "Test 47 Prediction: 0 True class: 2\n",
      "Test 48 Prediction: 0 True class: 4\n",
      "Test 49 Prediction: 0 True class: 4\n",
      "Test 50 Prediction: 0 True class: 6\n",
      "Test 51 Prediction: 0 True class: 3\n",
      "Test 52 Prediction: 0 True class: 5\n",
      "Test 53 Prediction: 0 True class: 5\n",
      "Test 54 Prediction: 0 True class: 6\n",
      "Test 55 Prediction: 1 True class: 0\n",
      "Test 56 Prediction: 0 True class: 4\n",
      "Test 57 Prediction: 0 True class: 1\n",
      "Test 58 Prediction: 0 True class: 9\n",
      "Test 59 Prediction: 0 True class: 5\n",
      "Test 60 Prediction: 0 True class: 7\n",
      "Test 61 Prediction: 0 True class: 8\n",
      "Test 62 Prediction: 0 True class: 9\n",
      "Test 63 Prediction: 0 True class: 3\n",
      "Test 64 Prediction: 0 True class: 7\n",
      "Test 65 Prediction: 0 True class: 4\n",
      "Test 66 Prediction: 0 True class: 6\n",
      "Test 67 Prediction: 0 True class: 4\n",
      "Test 68 Prediction: 0 True class: 3\n",
      "Test 69 Prediction: 1 True class: 0\n",
      "Test 70 Prediction: 0 True class: 7\n",
      "Test 71 Prediction: 1 True class: 0\n",
      "Test 72 Prediction: 0 True class: 2\n",
      "Test 73 Prediction: 0 True class: 9\n",
      "Test 74 Prediction: 0 True class: 1\n",
      "Test 75 Prediction: 0 True class: 7\n",
      "Test 76 Prediction: 0 True class: 3\n",
      "Test 77 Prediction: 0 True class: 2\n",
      "Test 78 Prediction: 0 True class: 9\n",
      "Test 79 Prediction: 0 True class: 7\n",
      "Test 80 Prediction: 0 True class: 7\n",
      "Test 81 Prediction: 0 True class: 6\n",
      "Test 82 Prediction: 0 True class: 2\n",
      "Test 83 Prediction: 0 True class: 7\n",
      "Test 84 Prediction: 0 True class: 8\n",
      "Test 85 Prediction: 0 True class: 4\n",
      "Test 86 Prediction: 0 True class: 7\n",
      "Test 87 Prediction: 0 True class: 3\n",
      "Test 88 Prediction: 0 True class: 6\n",
      "Test 89 Prediction: 0 True class: 1\n",
      "Test 90 Prediction: 0 True class: 3\n",
      "Test 91 Prediction: 0 True class: 6\n",
      "Test 92 Prediction: 0 True class: 9\n",
      "Test 93 Prediction: 0 True class: 3\n",
      "Test 94 Prediction: 0 True class: 1\n",
      "Test 95 Prediction: 0 True class: 4\n",
      "Test 96 Prediction: 0 True class: 1\n",
      "Test 97 Prediction: 0 True class: 7\n",
      "Test 98 Prediction: 0 True class: 6\n",
      "Test 99 Prediction: 0 True class: 9\n",
      "Test 100 Prediction: 0 True class: 6\n",
      "Test 101 Prediction: 1 True class: 0\n",
      "Test 102 Prediction: 0 True class: 5\n",
      "Test 103 Prediction: 0 True class: 4\n",
      "Test 104 Prediction: 0 True class: 9\n",
      "Test 105 Prediction: 0 True class: 9\n",
      "Test 106 Prediction: 0 True class: 2\n",
      "Test 107 Prediction: 0 True class: 1\n",
      "Test 108 Prediction: 0 True class: 9\n",
      "Test 109 Prediction: 0 True class: 4\n",
      "Test 110 Prediction: 0 True class: 8\n",
      "Test 111 Prediction: 0 True class: 7\n",
      "Test 112 Prediction: 0 True class: 3\n",
      "Test 113 Prediction: 0 True class: 9\n",
      "Test 114 Prediction: 0 True class: 7\n",
      "Test 115 Prediction: 0 True class: 4\n",
      "Test 116 Prediction: 0 True class: 4\n",
      "Test 117 Prediction: 0 True class: 4\n",
      "Test 118 Prediction: 0 True class: 9\n",
      "Test 119 Prediction: 0 True class: 2\n",
      "Test 120 Prediction: 0 True class: 5\n",
      "Test 121 Prediction: 0 True class: 4\n",
      "Test 122 Prediction: 0 True class: 7\n",
      "Test 123 Prediction: 0 True class: 6\n",
      "Test 124 Prediction: 0 True class: 7\n",
      "Test 125 Prediction: 0 True class: 9\n",
      "Test 126 Prediction: 1 True class: 0\n",
      "Test 127 Prediction: 0 True class: 5\n",
      "Test 128 Prediction: 0 True class: 8\n",
      "Test 129 Prediction: 0 True class: 5\n",
      "Test 130 Prediction: 0 True class: 6\n",
      "Test 131 Prediction: 0 True class: 6\n",
      "Test 132 Prediction: 0 True class: 5\n",
      "Test 133 Prediction: 0 True class: 7\n",
      "Test 134 Prediction: 0 True class: 8\n",
      "Test 135 Prediction: 0 True class: 1\n",
      "Test 136 Prediction: 1 True class: 0\n",
      "Test 137 Prediction: 0 True class: 1\n",
      "Test 138 Prediction: 0 True class: 6\n",
      "Test 139 Prediction: 0 True class: 4\n",
      "Test 140 Prediction: 0 True class: 6\n",
      "Test 141 Prediction: 0 True class: 7\n",
      "Test 142 Prediction: 0 True class: 3\n",
      "Test 143 Prediction: 0 True class: 1\n",
      "Test 144 Prediction: 0 True class: 7\n",
      "Test 145 Prediction: 0 True class: 1\n",
      "Test 146 Prediction: 0 True class: 8\n",
      "Test 147 Prediction: 0 True class: 2\n",
      "Test 148 Prediction: 1 True class: 0\n",
      "Test 149 Prediction: 0 True class: 2\n",
      "Test 150 Prediction: 0 True class: 9\n",
      "Test 151 Prediction: 0 True class: 9\n",
      "Test 152 Prediction: 0 True class: 5\n",
      "Test 153 Prediction: 0 True class: 5\n",
      "Test 154 Prediction: 0 True class: 1\n",
      "Test 155 Prediction: 0 True class: 5\n",
      "Test 156 Prediction: 0 True class: 6\n",
      "Test 157 Prediction: 1 True class: 0\n",
      "Test 158 Prediction: 0 True class: 3\n",
      "Test 159 Prediction: 0 True class: 4\n",
      "Test 160 Prediction: 0 True class: 4\n",
      "Test 161 Prediction: 0 True class: 6\n",
      "Test 162 Prediction: 0 True class: 5\n",
      "Test 163 Prediction: 0 True class: 4\n",
      "Test 164 Prediction: 0 True class: 6\n",
      "Test 165 Prediction: 0 True class: 5\n",
      "Test 166 Prediction: 0 True class: 4\n",
      "Test 167 Prediction: 0 True class: 5\n",
      "Test 168 Prediction: 0 True class: 1\n",
      "Test 169 Prediction: 0 True class: 4\n",
      "Test 170 Prediction: 0 True class: 4\n",
      "Test 171 Prediction: 0 True class: 7\n",
      "Test 172 Prediction: 0 True class: 2\n",
      "Test 173 Prediction: 0 True class: 3\n",
      "Test 174 Prediction: 0 True class: 2\n",
      "Test 175 Prediction: 0 True class: 7\n",
      "Test 176 Prediction: 0 True class: 1\n",
      "Test 177 Prediction: 0 True class: 8\n",
      "Test 178 Prediction: 0 True class: 1\n",
      "Test 179 Prediction: 0 True class: 8\n",
      "Test 180 Prediction: 0 True class: 1\n",
      "Test 181 Prediction: 0 True class: 8\n",
      "Test 182 Prediction: 0 True class: 5\n",
      "Test 183 Prediction: 1 True class: 0\n",
      "Test 184 Prediction: 0 True class: 8\n",
      "Test 185 Prediction: 0 True class: 9\n",
      "Test 186 Prediction: 0 True class: 2\n",
      "Test 187 Prediction: 0 True class: 5\n",
      "Test 188 Prediction: 1 True class: 0\n",
      "Test 189 Prediction: 0 True class: 1\n",
      "Test 190 Prediction: 0 True class: 1\n",
      "Test 191 Prediction: 0 True class: 1\n",
      "Test 192 Prediction: 1 True class: 0\n",
      "Test 193 Prediction: 0 True class: 9\n",
      "Test 194 Prediction: 1 True class: 0\n",
      "Test 195 Prediction: 0 True class: 3\n",
      "Test 196 Prediction: 0 True class: 1\n",
      "Test 197 Prediction: 0 True class: 6\n",
      "Test 198 Prediction: 0 True class: 4\n",
      "Test 199 Prediction: 0 True class: 2\n",
      "done\n",
      "accuracy: 0.92\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for i in range(len(Xte)):\n",
    "        nn_index = sess.run(pred,feed_dict={xtr:Xtr, xte:Xte[i,:]})\n",
    "        print \"Test\", i, \"Prediction:\", np.argmin(Ytr[[nn_index]]),\\\n",
    "        \"True class:\", np.argmax(Yte[i])\n",
    "        if np.argmax(Ytr[nn_index]) == np.argmax(Yte[i]):\n",
    "            accuracy += 1./len(Xte)\n",
    "    print \"done\"\n",
    "    print \"accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng=np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate=0.01\n",
    "training_epochs=1000\n",
    "display_step=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_X = np.asarray([3.3,4.4,5.5,6.71,6.93,4.168,9.779,6.182,7.59,2.167,7.042,10.791,5.313,7.997,5.654,9.27,3.1])\n",
    "train_Y = np.asarray([1.7,2.76,2.09,3.19,1.694,1.573,3.366,2.596,2.53,1.221, 2.827,3.465,1.65,2.904,2.42,2.94,1.3])\n",
    "n_samples = train_X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "W = tf.Variable(np.random.randn(),name=\"weight\")\n",
    "b = tf.Variable(np.random.randn(),name=\"bias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = tf.add(tf.multiply(X,W),b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = tf.reduce_sum(tf.pow(pred-Y,2))/(2*n_samples)\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0050 cost=0.262437880 W= 0.492575 b= -0.931777\n",
      "Epoch: 0050 cost=0.262163341 W= 0.496521 b= -0.930881\n",
      "Epoch: 0050 cost=0.262214571 W= 0.497459 b= -0.93071\n",
      "Epoch: 0050 cost=0.262691200 W= 0.500549 b= -0.93025\n",
      "Epoch: 0050 cost=0.262184858 W= 0.497106 b= -0.930746\n",
      "Epoch: 0050 cost=0.262262791 W= 0.498165 b= -0.930492\n",
      "Epoch: 0050 cost=0.262097597 W= 0.494857 b= -0.930831\n",
      "Epoch: 0050 cost=0.262099683 W= 0.496557 b= -0.930556\n",
      "Epoch: 0050 cost=0.262078911 W= 0.495181 b= -0.930737\n",
      "Epoch: 0050 cost=0.262007356 W= 0.496556 b= -0.930103\n",
      "Epoch: 0050 cost=0.262085229 W= 0.497634 b= -0.929949\n",
      "Epoch: 0050 cost=0.262294680 W= 0.491445 b= -0.930523\n",
      "Epoch: 0050 cost=0.262313634 W= 0.49135 b= -0.930541\n",
      "Epoch: 0050 cost=0.262401372 W= 0.490904 b= -0.930597\n",
      "Epoch: 0050 cost=0.262066960 W= 0.492816 b= -0.930258\n",
      "Epoch: 0050 cost=0.262834489 W= 0.489009 b= -0.930669\n",
      "Epoch: 0050 cost=0.262429416 W= 0.490313 b= -0.930249\n",
      "Epoch: 0100 cost=0.241005078 W= 0.478133 b= -0.828691\n",
      "Epoch: 0100 cost=0.240749702 W= 0.481976 b= -0.827818\n",
      "Epoch: 0100 cost=0.240795553 W= 0.482839 b= -0.827661\n",
      "Epoch: 0100 cost=0.241256356 W= 0.48591 b= -0.827203\n",
      "Epoch: 0100 cost=0.240765706 W= 0.48246 b= -0.827701\n",
      "Epoch: 0100 cost=0.240830004 W= 0.483416 b= -0.827472\n",
      "Epoch: 0100 cost=0.240691572 W= 0.480345 b= -0.827786\n",
      "Epoch: 0100 cost=0.240693823 W= 0.481997 b= -0.827518\n",
      "Epoch: 0100 cost=0.240674332 W= 0.480654 b= -0.827695\n",
      "Epoch: 0100 cost=0.240610182 W= 0.481938 b= -0.827103\n",
      "Epoch: 0100 cost=0.240683675 W= 0.483016 b= -0.82695\n",
      "Epoch: 0100 cost=0.240862340 W= 0.477174 b= -0.827491\n",
      "Epoch: 0100 cost=0.240896389 W= 0.476994 b= -0.827525\n",
      "Epoch: 0100 cost=0.240969718 W= 0.476603 b= -0.827574\n",
      "Epoch: 0100 cost=0.240665153 W= 0.478442 b= -0.827249\n",
      "Epoch: 0100 cost=0.241361752 W= 0.4748 b= -0.827642\n",
      "Epoch: 0100 cost=0.241007581 W= 0.475996 b= -0.827256\n",
      "Epoch: 0150 cost=0.222047389 W= 0.464549 b= -0.731735\n",
      "Epoch: 0150 cost=0.221809506 W= 0.468296 b= -0.730883\n",
      "Epoch: 0150 cost=0.221850410 W= 0.469089 b= -0.730739\n",
      "Epoch: 0150 cost=0.222296596 W= 0.472141 b= -0.730284\n",
      "Epoch: 0150 cost=0.221820712 W= 0.468686 b= -0.730783\n",
      "Epoch: 0150 cost=0.221873432 W= 0.469544 b= -0.730577\n",
      "Epoch: 0150 cost=0.221757993 W= 0.466697 b= -0.730868\n",
      "Epoch: 0150 cost=0.221760467 W= 0.468303 b= -0.730608\n",
      "Epoch: 0150 cost=0.221742094 W= 0.466991 b= -0.730781\n",
      "Epoch: 0150 cost=0.221684545 W= 0.468189 b= -0.730228\n",
      "Epoch: 0150 cost=0.221753970 W= 0.469267 b= -0.730075\n",
      "Epoch: 0150 cost=0.221905664 W= 0.463752 b= -0.730586\n",
      "Epoch: 0150 cost=0.221952260 W= 0.463492 b= -0.730635\n",
      "Epoch: 0150 cost=0.222012907 W= 0.463154 b= -0.730678\n",
      "Epoch: 0150 cost=0.221735224 W= 0.464923 b= -0.730365\n",
      "Epoch: 0150 cost=0.222368181 W= 0.461436 b= -0.730741\n",
      "Epoch: 0150 cost=0.222059041 W= 0.462531 b= -0.730388\n",
      "Epoch: 0200 cost=0.205279335 W= 0.451773 b= -0.640546\n",
      "Epoch: 0200 cost=0.205057427 W= 0.45543 b= -0.639715\n",
      "Epoch: 0200 cost=0.205093890 W= 0.456157 b= -0.639582\n",
      "Epoch: 0200 cost=0.205526397 W= 0.459191 b= -0.63913\n",
      "Epoch: 0200 cost=0.205064416 W= 0.45573 b= -0.63963\n",
      "Epoch: 0200 cost=0.205107346 W= 0.456498 b= -0.639445\n",
      "Epoch: 0200 cost=0.205011666 W= 0.45386 b= -0.639715\n",
      "Epoch: 0200 cost=0.205014348 W= 0.455423 b= -0.639462\n",
      "Epoch: 0200 cost=0.204996988 W= 0.454141 b= -0.639631\n",
      "Epoch: 0200 cost=0.204945356 W= 0.455258 b= -0.639116\n",
      "Epoch: 0200 cost=0.205010936 W= 0.456336 b= -0.638963\n",
      "Epoch: 0200 cost=0.205139086 W= 0.451129 b= -0.639445\n",
      "Epoch: 0200 cost=0.205196023 W= 0.450793 b= -0.639508\n",
      "Epoch: 0200 cost=0.205245614 W= 0.450504 b= -0.639545\n",
      "Epoch: 0200 cost=0.204992086 W= 0.452208 b= -0.639243\n",
      "Epoch: 0200 cost=0.205567956 W= 0.448867 b= -0.639604\n",
      "Epoch: 0200 cost=0.205298588 W= 0.449866 b= -0.639281\n",
      "Epoch: 0250 cost=0.190448046 W= 0.439757 b= -0.55478\n",
      "Epoch: 0250 cost=0.190240592 W= 0.443328 b= -0.553968\n",
      "Epoch: 0250 cost=0.190273121 W= 0.443994 b= -0.553847\n",
      "Epoch: 0250 cost=0.190692842 W= 0.447012 b= -0.553397\n",
      "Epoch: 0250 cost=0.190244079 W= 0.443545 b= -0.553898\n",
      "Epoch: 0250 cost=0.190278590 W= 0.444227 b= -0.553734\n",
      "Epoch: 0250 cost=0.190199941 W= 0.441786 b= -0.553983\n",
      "Epoch: 0250 cost=0.190202773 W= 0.44331 b= -0.553737\n",
      "Epoch: 0250 cost=0.190186366 W= 0.442055 b= -0.553902\n",
      "Epoch: 0250 cost=0.190140039 W= 0.443096 b= -0.553422\n",
      "Epoch: 0250 cost=0.190201968 W= 0.444174 b= -0.553269\n",
      "Epoch: 0250 cost=0.190309659 W= 0.439256 b= -0.553725\n",
      "Epoch: 0250 cost=0.190375015 W= 0.438849 b= -0.553801\n",
      "Epoch: 0250 cost=0.190414846 W= 0.438606 b= -0.553831\n",
      "Epoch: 0250 cost=0.190183118 W= 0.440249 b= -0.553541\n",
      "Epoch: 0250 cost=0.190707773 W= 0.437045 b= -0.553886\n",
      "Epoch: 0250 cost=0.190473601 W= 0.437955 b= -0.553593\n",
      "Epoch: 0300 cost=0.177329868 W= 0.428456 b= -0.474115\n",
      "Epoch: 0300 cost=0.177135661 W= 0.431947 b= -0.473321\n",
      "Epoch: 0300 cost=0.177164614 W= 0.432554 b= -0.473211\n",
      "Epoch: 0300 cost=0.177572444 W= 0.435557 b= -0.472763\n",
      "Epoch: 0300 cost=0.177136078 W= 0.432085 b= -0.473264\n",
      "Epoch: 0300 cost=0.177163601 W= 0.432687 b= -0.47312\n",
      "Epoch: 0300 cost=0.177099437 W= 0.430431 b= -0.473351\n",
      "Epoch: 0300 cost=0.177102432 W= 0.431916 b= -0.47311\n",
      "Epoch: 0300 cost=0.177086875 W= 0.430688 b= -0.473272\n",
      "Epoch: 0300 cost=0.177045330 W= 0.431658 b= -0.472825\n",
      "Epoch: 0300 cost=0.177103847 W= 0.432735 b= -0.472672\n",
      "Epoch: 0300 cost=0.177193746 W= 0.428089 b= -0.473102\n",
      "Epoch: 0300 cost=0.177265868 W= 0.427616 b= -0.473191\n",
      "Epoch: 0300 cost=0.177297205 W= 0.427416 b= -0.473216\n",
      "Epoch: 0300 cost=0.177084997 W= 0.429001 b= -0.472936\n",
      "Epoch: 0300 cost=0.177563593 W= 0.425927 b= -0.473268\n",
      "Epoch: 0300 cost=0.177360564 W= 0.426752 b= -0.473001\n",
      "Epoch: 0350 cost=0.165727019 W= 0.417827 b= -0.398247\n",
      "Epoch: 0350 cost=0.165544957 W= 0.421243 b= -0.397471\n",
      "Epoch: 0350 cost=0.165570587 W= 0.421795 b= -0.39737\n",
      "Epoch: 0350 cost=0.165967345 W= 0.424783 b= -0.396925\n",
      "Epoch: 0350 cost=0.165542692 W= 0.421307 b= -0.397427\n",
      "Epoch: 0350 cost=0.165564314 W= 0.421832 b= -0.3973\n",
      "Epoch: 0350 cost=0.165512532 W= 0.419751 b= -0.397513\n",
      "Epoch: 0350 cost=0.165515646 W= 0.421201 b= -0.397279\n",
      "Epoch: 0350 cost=0.165500879 W= 0.419997 b= -0.397437\n",
      "Epoch: 0350 cost=0.165463567 W= 0.4209 b= -0.397021\n",
      "Epoch: 0350 cost=0.165518880 W= 0.421977 b= -0.396868\n",
      "Epoch: 0350 cost=0.165593281 W= 0.417586 b= -0.397275\n",
      "Epoch: 0350 cost=0.165670782 W= 0.417051 b= -0.397375\n",
      "Epoch: 0350 cost=0.165694699 W= 0.416892 b= -0.397395\n",
      "Epoch: 0350 cost=0.165500164 W= 0.418423 b= -0.397125\n",
      "Epoch: 0350 cost=0.165937409 W= 0.415469 b= -0.397443\n",
      "Epoch: 0350 cost=0.165761828 W= 0.416216 b= -0.397202\n",
      "Epoch: 0400 cost=0.155464590 W= 0.40783 b= -0.326892\n",
      "Epoch: 0400 cost=0.155293569 W= 0.411175 b= -0.326131\n",
      "Epoch: 0400 cost=0.155316263 W= 0.411675 b= -0.326041\n",
      "Epoch: 0400 cost=0.155702695 W= 0.41465 b= -0.325597\n",
      "Epoch: 0400 cost=0.155289054 W= 0.411169 b= -0.326099\n",
      "Epoch: 0400 cost=0.155305743 W= 0.411624 b= -0.32599\n",
      "Epoch: 0400 cost=0.155264497 W= 0.409706 b= -0.326187\n",
      "Epoch: 0400 cost=0.155267730 W= 0.411122 b= -0.325957\n",
      "Epoch: 0400 cost=0.155253679 W= 0.409942 b= -0.326113\n",
      "Epoch: 0400 cost=0.155220181 W= 0.410781 b= -0.325725\n",
      "Epoch: 0400 cost=0.155272484 W= 0.411858 b= -0.325573\n",
      "Epoch: 0400 cost=0.155333400 W= 0.407708 b= -0.325957\n",
      "Epoch: 0400 cost=0.155415118 W= 0.407114 b= -0.326069\n",
      "Epoch: 0400 cost=0.155432507 W= 0.406993 b= -0.326084\n",
      "Epoch: 0400 cost=0.155253947 W= 0.408473 b= -0.325822\n",
      "Epoch: 0400 cost=0.155653894 W= 0.405634 b= -0.326129\n",
      "Epoch: 0400 cost=0.155502632 W= 0.406306 b= -0.325912\n",
      "Epoch: 0450 cost=0.146387756 W= 0.398427 b= -0.25978\n",
      "Epoch: 0450 cost=0.146226853 W= 0.401706 b= -0.259035\n",
      "Epoch: 0450 cost=0.146246865 W= 0.402158 b= -0.258953\n",
      "Epoch: 0450 cost=0.146623626 W= 0.40512 b= -0.258512\n",
      "Epoch: 0450 cost=0.146220401 W= 0.401635 b= -0.259015\n",
      "Epoch: 0450 cost=0.146233067 W= 0.402022 b= -0.258922\n",
      "Epoch: 0450 cost=0.146200702 W= 0.400259 b= -0.259102\n",
      "Epoch: 0450 cost=0.146204025 W= 0.401644 b= -0.258878\n",
      "Epoch: 0450 cost=0.146190614 W= 0.400485 b= -0.259031\n",
      "Epoch: 0450 cost=0.146160558 W= 0.401265 b= -0.258671\n",
      "Epoch: 0450 cost=0.146210015 W= 0.402342 b= -0.258518\n",
      "Epoch: 0450 cost=0.146259323 W= 0.398418 b= -0.258881\n",
      "Epoch: 0450 cost=0.146344140 W= 0.397768 b= -0.259004\n",
      "Epoch: 0450 cost=0.146355838 W= 0.397684 b= -0.259014\n",
      "Epoch: 0450 cost=0.146191657 W= 0.399116 b= -0.258761\n",
      "Epoch: 0450 cost=0.146558121 W= 0.396383 b= -0.259056\n",
      "Epoch: 0450 cost=0.146428362 W= 0.396986 b= -0.258861\n",
      "Epoch: 0500 cost=0.138359591 W= 0.389584 b= -0.19666\n",
      "Epoch: 0500 cost=0.138208002 W= 0.3928 b= -0.195929\n",
      "Epoch: 0500 cost=0.138225555 W= 0.393206 b= -0.195856\n",
      "Epoch: 0500 cost=0.138593286 W= 0.396156 b= -0.195416\n",
      "Epoch: 0500 cost=0.138199881 W= 0.392667 b= -0.195919\n",
      "Epoch: 0500 cost=0.138209268 W= 0.392991 b= -0.195842\n",
      "Epoch: 0500 cost=0.138184339 W= 0.391374 b= -0.196007\n",
      "Epoch: 0500 cost=0.138187766 W= 0.392728 b= -0.195788\n",
      "Epoch: 0500 cost=0.138174966 W= 0.39159 b= -0.195938\n",
      "Epoch: 0500 cost=0.138147950 W= 0.392314 b= -0.195604\n",
      "Epoch: 0500 cost=0.138194785 W= 0.393391 b= -0.195451\n",
      "Epoch: 0500 cost=0.138233960 W= 0.38968 b= -0.195795\n",
      "Epoch: 0500 cost=0.138321102 W= 0.388978 b= -0.195927\n",
      "Epoch: 0500 cost=0.138327792 W= 0.388928 b= -0.195933\n",
      "Epoch: 0500 cost=0.138176620 W= 0.390314 b= -0.195688\n",
      "Epoch: 0500 cost=0.138512865 W= 0.387683 b= -0.195972\n",
      "Epoch: 0500 cost=0.138402089 W= 0.388219 b= -0.195799\n",
      "Epoch: 0550 cost=0.131259024 W= 0.381267 b= -0.137294\n",
      "Epoch: 0550 cost=0.131115898 W= 0.384424 b= -0.136577\n",
      "Epoch: 0550 cost=0.131131336 W= 0.384787 b= -0.136511\n",
      "Epoch: 0550 cost=0.131490618 W= 0.387726 b= -0.136073\n",
      "Epoch: 0550 cost=0.131106496 W= 0.384233 b= -0.136577\n",
      "Epoch: 0550 cost=0.131113172 W= 0.384498 b= -0.136513\n",
      "Epoch: 0550 cost=0.131094441 W= 0.383017 b= -0.136665\n",
      "Epoch: 0550 cost=0.131097928 W= 0.384344 b= -0.13645\n",
      "Epoch: 0550 cost=0.131085753 W= 0.383224 b= -0.136598\n",
      "Epoch: 0550 cost=0.131061494 W= 0.383896 b= -0.136288\n",
      "Epoch: 0550 cost=0.131105825 W= 0.384973 b= -0.136135\n",
      "Epoch: 0550 cost=0.131136253 W= 0.381462 b= -0.13646\n",
      "Epoch: 0550 cost=0.131224915 W= 0.380711 b= -0.136601\n",
      "Epoch: 0550 cost=0.131227270 W= 0.380692 b= -0.136604\n",
      "Epoch: 0550 cost=0.131087929 W= 0.382037 b= -0.136366\n",
      "Epoch: 0550 cost=0.131396875 W= 0.3795 b= -0.13664\n",
      "Epoch: 0550 cost=0.131302923 W= 0.379975 b= -0.136487\n",
      "Epoch: 0600 cost=0.124978960 W= 0.373444 b= -0.0814593\n",
      "Epoch: 0600 cost=0.124843657 W= 0.376546 b= -0.0807544\n",
      "Epoch: 0600 cost=0.124857076 W= 0.376869 b= -0.0806957\n",
      "Epoch: 0600 cost=0.125208497 W= 0.379797 b= -0.0802593\n",
      "Epoch: 0600 cost=0.124833025 W= 0.3763 b= -0.0807638\n",
      "Epoch: 0600 cost=0.124837592 W= 0.37651 b= -0.0807136\n",
      "Epoch: 0600 cost=0.124824062 W= 0.375157 b= -0.080852\n",
      "Epoch: 0600 cost=0.124827668 W= 0.376457 b= -0.0806416\n",
      "Epoch: 0600 cost=0.124815956 W= 0.375356 b= -0.0807867\n",
      "Epoch: 0600 cost=0.124794178 W= 0.375979 b= -0.0804994\n",
      "Epoch: 0600 cost=0.124836124 W= 0.377055 b= -0.0803465\n",
      "Epoch: 0600 cost=0.124859110 W= 0.373732 b= -0.0806545\n",
      "Epoch: 0600 cost=0.124948643 W= 0.372935 b= -0.0808044\n",
      "Epoch: 0600 cost=0.124947250 W= 0.372947 b= -0.080803\n",
      "Epoch: 0600 cost=0.124818504 W= 0.374251 b= -0.0805723\n",
      "Epoch: 0600 cost=0.125102878 W= 0.371804 b= -0.0808363\n",
      "Epoch: 0600 cost=0.125023752 W= 0.37222 b= -0.080702\n",
      "Epoch: 0650 cost=0.119424596 W= 0.366087 b= -0.0289448\n",
      "Epoch: 0650 cost=0.119296454 W= 0.369136 b= -0.0282517\n",
      "Epoch: 0650 cost=0.119308077 W= 0.369421 b= -0.0282\n",
      "Epoch: 0650 cost=0.119652100 W= 0.372339 b= -0.027765\n",
      "Epoch: 0650 cost=0.119284898 W= 0.36884 b= -0.0282701\n",
      "Epoch: 0650 cost=0.119287796 W= 0.368996 b= -0.0282325\n",
      "Epoch: 0650 cost=0.119278513 W= 0.367764 b= -0.0283584\n",
      "Epoch: 0650 cost=0.119282179 W= 0.36904 b= -0.0281521\n",
      "Epoch: 0650 cost=0.119270928 W= 0.367956 b= -0.0282949\n",
      "Epoch: 0650 cost=0.119251408 W= 0.368532 b= -0.0280291\n",
      "Epoch: 0650 cost=0.119291127 W= 0.369608 b= -0.0278762\n",
      "Epoch: 0650 cost=0.119307630 W= 0.366463 b= -0.0281678\n",
      "Epoch: 0650 cost=0.119397528 W= 0.365622 b= -0.0283259\n",
      "Epoch: 0650 cost=0.119392827 W= 0.365662 b= -0.0283209\n",
      "Epoch: 0650 cost=0.119273789 W= 0.366929 b= -0.0280969\n",
      "Epoch: 0650 cost=0.119535938 W= 0.364566 b= -0.0283518\n",
      "Epoch: 0650 cost=0.119469911 W= 0.364927 b= -0.0282352\n",
      "Epoch: 0700 cost=0.114512183 W= 0.359167 b= 0.0204463\n",
      "Epoch: 0700 cost=0.114390582 W= 0.362167 b= 0.0211282\n",
      "Epoch: 0700 cost=0.114400566 W= 0.362416 b= 0.0211735\n",
      "Epoch: 0700 cost=0.114737719 W= 0.365325 b= 0.021607\n",
      "Epoch: 0700 cost=0.114378244 W= 0.361823 b= 0.0211016\n",
      "Epoch: 0700 cost=0.114379913 W= 0.36193 b= 0.0211273\n",
      "Epoch: 0700 cost=0.114374079 W= 0.360811 b= 0.021013\n",
      "Epoch: 0700 cost=0.114377782 W= 0.362064 b= 0.0212156\n",
      "Epoch: 0700 cost=0.114366964 W= 0.360996 b= 0.0210748\n",
      "Epoch: 0700 cost=0.114349462 W= 0.361528 b= 0.0213205\n",
      "Epoch: 0700 cost=0.114387102 W= 0.362604 b= 0.0214733\n",
      "Epoch: 0700 cost=0.114398070 W= 0.359625 b= 0.0211972\n",
      "Epoch: 0700 cost=0.114487909 W= 0.358744 b= 0.0210314\n",
      "Epoch: 0700 cost=0.114480339 W= 0.35881 b= 0.0210397\n",
      "Epoch: 0700 cost=0.114370048 W= 0.360042 b= 0.0212575\n",
      "Epoch: 0700 cost=0.114612095 W= 0.357758 b= 0.0210111\n",
      "Epoch: 0700 cost=0.114557683 W= 0.358068 b= 0.0211111\n",
      "Epoch: 0750 cost=0.110167496 W= 0.352659 b= 0.0669001\n",
      "Epoch: 0750 cost=0.110051982 W= 0.355613 b= 0.0675715\n",
      "Epoch: 0750 cost=0.110060468 W= 0.355828 b= 0.0676106\n",
      "Epoch: 0750 cost=0.110391140 W= 0.358729 b= 0.0680428\n",
      "Epoch: 0750 cost=0.110038973 W= 0.355223 b= 0.0675369\n",
      "Epoch: 0750 cost=0.110039763 W= 0.355284 b= 0.0675516\n",
      "Epoch: 0750 cost=0.110036679 W= 0.354272 b= 0.0674481\n",
      "Epoch: 0750 cost=0.110040411 W= 0.355503 b= 0.0676472\n",
      "Epoch: 0750 cost=0.110030018 W= 0.35445 b= 0.0675084\n",
      "Epoch: 0750 cost=0.110014327 W= 0.354941 b= 0.0677351\n",
      "Epoch: 0750 cost=0.110050008 W= 0.356017 b= 0.0678879\n",
      "Epoch: 0750 cost=0.110056236 W= 0.353194 b= 0.0676264\n",
      "Epoch: 0750 cost=0.110145628 W= 0.352275 b= 0.0674534\n",
      "Epoch: 0750 cost=0.110135585 W= 0.352366 b= 0.0674648\n",
      "Epoch: 0750 cost=0.110033259 W= 0.353565 b= 0.0676767\n",
      "Epoch: 0750 cost=0.110257111 W= 0.351355 b= 0.0674383\n",
      "Epoch: 0750 cost=0.110212907 W= 0.351616 b= 0.0675227\n",
      "Epoch: 0800 cost=0.106325030 W= 0.346538 b= 0.110591\n",
      "Epoch: 0800 cost=0.106215090 W= 0.349449 b= 0.111252\n",
      "Epoch: 0800 cost=0.106222205 W= 0.349632 b= 0.111286\n",
      "Epoch: 0800 cost=0.106546871 W= 0.352524 b= 0.111717\n",
      "Epoch: 0800 cost=0.106201567 W= 0.349016 b= 0.111211\n",
      "Epoch: 0800 cost=0.106201746 W= 0.349033 b= 0.111215\n",
      "Epoch: 0800 cost=0.106200844 W= 0.348122 b= 0.111122\n",
      "Epoch: 0800 cost=0.106204584 W= 0.349332 b= 0.111317\n",
      "Epoch: 0800 cost=0.106194586 W= 0.348293 b= 0.11118\n",
      "Epoch: 0800 cost=0.106180504 W= 0.348745 b= 0.111389\n",
      "Epoch: 0800 cost=0.106214352 W= 0.349821 b= 0.111542\n",
      "Epoch: 0800 cost=0.106216572 W= 0.347146 b= 0.111294\n",
      "Epoch: 0800 cost=0.106305189 W= 0.346191 b= 0.111114\n",
      "Epoch: 0800 cost=0.106292993 W= 0.346305 b= 0.111129\n",
      "Epoch: 0800 cost=0.106197923 W= 0.347472 b= 0.111335\n",
      "Epoch: 0800 cost=0.106405318 W= 0.345333 b= 0.111104\n",
      "Epoch: 0800 cost=0.106370166 W= 0.345548 b= 0.111174\n",
      "Epoch: 0850 cost=0.102926768 W= 0.340781 b= 0.151683\n",
      "Epoch: 0850 cost=0.102821983 W= 0.343651 b= 0.152336\n",
      "Epoch: 0850 cost=0.102827847 W= 0.343805 b= 0.152364\n",
      "Epoch: 0850 cost=0.103146851 W= 0.346689 b= 0.152793\n",
      "Epoch: 0850 cost=0.102808036 W= 0.343178 b= 0.152287\n",
      "Epoch: 0850 cost=0.102807827 W= 0.343154 b= 0.152281\n",
      "Epoch: 0850 cost=0.102808595 W= 0.342337 b= 0.152197\n",
      "Epoch: 0850 cost=0.102812380 W= 0.343528 b= 0.15239\n",
      "Epoch: 0850 cost=0.102802724 W= 0.342502 b= 0.152255\n",
      "Epoch: 0850 cost=0.102790140 W= 0.342918 b= 0.152447\n",
      "Epoch: 0850 cost=0.102822244 W= 0.343994 b= 0.1526\n",
      "Epoch: 0850 cost=0.102821030 W= 0.341458 b= 0.152365\n",
      "Epoch: 0850 cost=0.102908634 W= 0.340468 b= 0.152179\n",
      "Epoch: 0850 cost=0.102894597 W= 0.340605 b= 0.152196\n",
      "Epoch: 0850 cost=0.102806099 W= 0.341743 b= 0.152397\n",
      "Epoch: 0850 cost=0.102998547 W= 0.339669 b= 0.152173\n",
      "Epoch: 0850 cost=0.102971397 W= 0.339842 b= 0.152229\n",
      "Epoch: 0900 cost=0.099921398 W= 0.335366 b= 0.190332\n",
      "Epoch: 0900 cost=0.099821329 W= 0.338198 b= 0.190975\n",
      "Epoch: 0900 cost=0.099826060 W= 0.338324 b= 0.190998\n",
      "Epoch: 0900 cost=0.100139782 W= 0.341201 b= 0.191427\n",
      "Epoch: 0900 cost=0.099807054 W= 0.337687 b= 0.19092\n",
      "Epoch: 0900 cost=0.099806733 W= 0.337625 b= 0.190905\n",
      "Epoch: 0900 cost=0.099808730 W= 0.336897 b= 0.19083\n",
      "Epoch: 0900 cost=0.099812552 W= 0.338069 b= 0.19102\n",
      "Epoch: 0900 cost=0.099803194 W= 0.337056 b= 0.190886\n",
      "Epoch: 0900 cost=0.099791944 W= 0.337438 b= 0.191063\n",
      "Epoch: 0900 cost=0.099822424 W= 0.338514 b= 0.191216\n",
      "Epoch: 0900 cost=0.099818341 W= 0.336107 b= 0.190993\n",
      "Epoch: 0900 cost=0.099904723 W= 0.335086 b= 0.1908\n",
      "Epoch: 0900 cost=0.099889070 W= 0.335244 b= 0.19082\n",
      "Epoch: 0900 cost=0.099806577 W= 0.336354 b= 0.191016\n",
      "Epoch: 0900 cost=0.099985503 W= 0.334342 b= 0.190799\n",
      "Epoch: 0900 cost=0.099965371 W= 0.334474 b= 0.190842\n",
      "Epoch: 0950 cost=0.097263522 W= 0.330273 b= 0.226681\n",
      "Epoch: 0950 cost=0.097167857 W= 0.333069 b= 0.227316\n",
      "Epoch: 0950 cost=0.097171530 W= 0.333169 b= 0.227335\n",
      "Epoch: 0950 cost=0.097480290 W= 0.336039 b= 0.227762\n",
      "Epoch: 0950 cost=0.097153328 W= 0.332523 b= 0.227255\n",
      "Epoch: 0950 cost=0.097153015 W= 0.332424 b= 0.227231\n",
      "Epoch: 0950 cost=0.097155899 W= 0.33178 b= 0.227165\n",
      "Epoch: 0950 cost=0.097159721 W= 0.332935 b= 0.227352\n",
      "Epoch: 0950 cost=0.097150691 W= 0.331934 b= 0.22722\n",
      "Epoch: 0950 cost=0.097140618 W= 0.332284 b= 0.227382\n",
      "Epoch: 0950 cost=0.097169563 W= 0.333359 b= 0.227535\n",
      "Epoch: 0950 cost=0.097163036 W= 0.331075 b= 0.227323\n",
      "Epoch: 0950 cost=0.097248070 W= 0.330024 b= 0.227125\n",
      "Epoch: 0950 cost=0.097231060 W= 0.330201 b= 0.227147\n",
      "Epoch: 0950 cost=0.097154006 W= 0.331285 b= 0.227339\n",
      "Epoch: 0950 cost=0.097320624 W= 0.329331 b= 0.227128\n",
      "Epoch: 0950 cost=0.097306766 W= 0.329426 b= 0.227159\n",
      "Epoch: 1000 cost=0.094913006 W= 0.325484 b= 0.260869\n",
      "Epoch: 1000 cost=0.094821356 W= 0.328245 b= 0.261497\n",
      "Epoch: 1000 cost=0.094824106 W= 0.32832 b= 0.26151\n",
      "Epoch: 1000 cost=0.095128216 W= 0.331184 b= 0.261937\n",
      "Epoch: 1000 cost=0.094806641 W= 0.327665 b= 0.261429\n",
      "Epoch: 1000 cost=0.094806530 W= 0.327533 b= 0.261397\n",
      "Epoch: 1000 cost=0.094809957 W= 0.326967 b= 0.261339\n",
      "Epoch: 1000 cost=0.094813794 W= 0.328107 b= 0.261524\n",
      "Epoch: 1000 cost=0.094805032 W= 0.327116 b= 0.261393\n",
      "Epoch: 1000 cost=0.094796076 W= 0.327436 b= 0.261541\n",
      "Epoch: 1000 cost=0.094823577 W= 0.328511 b= 0.261694\n",
      "Epoch: 1000 cost=0.094815038 W= 0.326343 b= 0.261493\n",
      "Epoch: 1000 cost=0.094898582 W= 0.325263 b= 0.261289\n",
      "Epoch: 1000 cost=0.094880365 W= 0.325459 b= 0.261314\n",
      "Epoch: 1000 cost=0.094808318 W= 0.326518 b= 0.261501\n",
      "Epoch: 1000 cost=0.094963729 W= 0.324619 b= 0.261296\n",
      "Epoch: 1000 cost=0.094955362 W= 0.324678 b= 0.261315\n",
      "Optimizatin finished\n",
      "Training cost= 0.0949554 W= 0.324678 b= 0.261315 \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f08310363d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(1000):\n",
    "        for (x,y) in zip (train_X,train_Y):\n",
    "            sess.run(optimizer,feed_dict={X:x,Y:y})\n",
    "            if (epoch+1) %50 == 0:\n",
    "                c = sess.run(cost,feed_dict={X:train_X,Y:train_Y})\n",
    "                print \"Epoch:\",'%04d' % (epoch+1), \"cost={:.9f}\".format(c),\\\n",
    "                \"W=\",sess.run(W),\"b=\",sess.run(b)\n",
    "    print \"Optimizatin finished\"\n",
    "    training_cost = sess.run(cost,feed_dict={X:train_X,Y:train_Y})\n",
    "    print \"Training cost=\",training_cost, \"W=\",sess.run(W),\"b=\",sess.run(b),'\\n'\n",
    "    \n",
    "    plt.plot(train_X,train_Y,'ro',label='Original data')\n",
    "    plt.plot(train_X,sess.run(W)*train_X + sess.run(b),label='Fitted line')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regression result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y = tf.placeholder(tf.float32, [None,10])\n",
    "\n",
    "W = tf.Variable(tf.zeros([784,10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = tf.nn.softmax(tf.matmul(x,W) + b)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(y*tf.log(pred),reduction_indices=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 1.182557052\n",
      "Epoch: 0002 cost= 0.664481427\n",
      "Epoch: 0003 cost= 0.552245180\n",
      "Epoch: 0004 cost= 0.498443319\n",
      "Epoch: 0005 cost= 0.464838493\n",
      "Epoch: 0006 cost= 0.442337350\n",
      "Epoch: 0007 cost= 0.425509836\n",
      "Epoch: 0008 cost= 0.411367112\n",
      "Epoch: 0009 cost= 0.400994322\n",
      "Epoch: 0010 cost= 0.391949009\n",
      "Epoch: 0011 cost= 0.384607156\n",
      "Epoch: 0012 cost= 0.377845663\n",
      "Epoch: 0013 cost= 0.371765205\n",
      "Epoch: 0014 cost= 0.366445094\n",
      "Epoch: 0015 cost= 0.362732655\n",
      "Epoch: 0016 cost= 0.357974668\n",
      "Epoch: 0017 cost= 0.354613680\n",
      "Epoch: 0018 cost= 0.351272013\n",
      "Epoch: 0019 cost= 0.347940990\n",
      "Epoch: 0020 cost= 0.345177150\n",
      "Epoch: 0021 cost= 0.342402279\n",
      "Epoch: 0022 cost= 0.339781418\n",
      "Epoch: 0023 cost= 0.337926834\n",
      "Epoch: 0024 cost= 0.334914316\n",
      "Epoch: 0025 cost= 0.333512835\n",
      "Optimization Finished\n",
      "Accuracy: 0.889333\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(25):\n",
    "        avg_cost = 0\n",
    "        total_batch = int(mnist.train.num_examples/100)\n",
    "        for i in range(total_batch):\n",
    "            batch_xs,batch_ys= mnist.train.next_batch(total_batch)\n",
    "            _,c = sess.run([optimizer,cost],feed_dict={x:batch_xs,y:batch_ys})\n",
    "            avg_cost += c / total_batch\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "    print \"Optimization Finished\"\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmbbax(pred,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    print \"Accuracy:\", accuracy.eval({x:mnist.test.images[:3000],y:mnist.test.labels[:3000]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_hidden_1=256\n",
    "n_hidden_2=256\n",
    "n_input=784\n",
    "n_classes=10\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x= tf.placeholder(\"float\",[None,n_input])\n",
    "y= tf.placeholder(\"float\",[None,n_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multilayer_perceptron(x, weights, biases):\n",
    "    layer_1 = tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    layer_2 = tf.add(tf.matmul(layer_1,weights['h2']),biases['b2'])\n",
    "    layer_2 = tf.nn.relu(layer_2)\n",
    "    out_layer = tf.matmul(layer_2,weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights={\n",
    "    'h1': tf.Variable(tf.random_normal([n_input,n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1,n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2,n_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = multilayer_perceptron(x,weights,biases)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(cost)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost= 150.806635838\n",
      "Epoch: 0002 cost= 39.552191466\n",
      "Epoch: 0003 cost= 25.048706896\n",
      "Epoch: 0004 cost= 18.122025902\n",
      "Epoch: 0005 cost= 12.714621966\n",
      "Epoch: 0006 cost= 9.826274027\n",
      "Epoch: 0007 cost= 6.539884287\n",
      "Epoch: 0008 cost= 5.704044151\n",
      "Epoch: 0009 cost= 3.904377537\n",
      "Epoch: 0010 cost= 2.957406140\n",
      "Epoch: 0011 cost= 2.255832502\n",
      "Epoch: 0012 cost= 1.634729282\n",
      "Epoch: 0013 cost= 1.293409503\n",
      "Epoch: 0014 cost= 1.025933049\n",
      "Epoch: 0015 cost= 0.797613138\n",
      "Optimization Finished\n",
      "Accuracy: 0.9447\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for epoch in range(15):\n",
    "        avg_cost =0\n",
    "        total_batch = int(mnist.train.num_examples/100)\n",
    "        for i in range(total_batch):\n",
    "            batch_x, batch_y = mnist.train.next_batch(100)\n",
    "            _,c = sess.run([optimizer,cost],feed_dict={x:batch_x,y:batch_y})\n",
    "            avg_cost += c /total_batch\n",
    "        if epoch % 1 == 0:\n",
    "            print \"Epoch:\", '%04d' % (epoch+1), \"cost=\",\\\n",
    "            \"{:.9f}\".format(avg_cost)\n",
    "    print 'Optimization Finished'\n",
    "    \n",
    "    correct_prediction = tf.equal(tf.argmax(pred,1),tf.argmax(y,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction,\"float\"))\n",
    "    print \"Accuracy:\",accuracy.eval({x:mnist.test.images, y:mnist.test.labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n",
       "       ..., \n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dropout=0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,n_input])\n",
    "y = tf.placeholder(tf.float32,[None,n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x,W,b,strides=1):\n",
    "    x = tf.nn.conv2d(x,W,strides=[1,strides,strides,1],padding='SAME')\n",
    "    x = tf.nn.bias_add(x,b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "def maxpool2d(x,k=2):\n",
    "    return tf.nn.max_pool(x,ksize=[1,k,k,1],strides=[1,k,k,1],padding='SAME')\n",
    "\n",
    "def conv_net(x,weights,biases,dropout):\n",
    "    x = tf.reshape(x, shape=[-1,28,28,1])\n",
    "    conv1 = conv2d(x,weights['wc1'],biases['bc1'])\n",
    "    conv1 = maxpool2d(conv1,k=2)\n",
    "    \n",
    "    conv2 = conv2d(conv1, weights['wc2'],biases['bc2'])\n",
    "    conv2 = maxpool2d(conv2,k=2)\n",
    "    \n",
    "    fc1 = tf.reshape(conv2,[-1,weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']),biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    \n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "    out = tf.add(tf.matmul(fc1,weights['out']),biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5,5,1,32])),\n",
    "    'wc2': tf.Variable(tf.random_normal([5,5,32,64])),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*64,1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024,n_classes]))\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([32])),\n",
    "    'bc2': tf.Variable(tf.random_normal([64])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}\n",
    "\n",
    "pred = conv_net(x, weights,biases,keep_prob)\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred,labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate= 0.001).minimize(cost)\n",
    "\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred,tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1280, minibatch loss=31941.031250, Training accuracy=0.23438\n",
      "Iter 2560, minibatch loss=13611.220703, Training accuracy=0.42969\n",
      "Iter 3840, minibatch loss=8440.535156, Training accuracy=0.64062\n",
      "Iter 5120, minibatch loss=6561.283691, Training accuracy=0.67188\n",
      "Iter 6400, minibatch loss=3944.933350, Training accuracy=0.73438\n",
      "Iter 7680, minibatch loss=2905.489258, Training accuracy=0.84375\n",
      "Iter 8960, minibatch loss=4012.606201, Training accuracy=0.81250\n",
      "Iter 10240, minibatch loss=1295.185425, Training accuracy=0.92188\n",
      "Iter 11520, minibatch loss=3532.875488, Training accuracy=0.85156\n",
      "Iter 12800, minibatch loss=1203.858398, Training accuracy=0.92969\n",
      "Iter 14080, minibatch loss=3558.112305, Training accuracy=0.85156\n",
      "Iter 15360, minibatch loss=3356.121826, Training accuracy=0.85938\n",
      "Iter 16640, minibatch loss=1682.414307, Training accuracy=0.90625\n",
      "Iter 17920, minibatch loss=227.095612, Training accuracy=0.96875\n",
      "Iter 19200, minibatch loss=1047.295044, Training accuracy=0.95312\n",
      "Optimization Finished\n",
      "Testing Accuracy:"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 256 is out of bounds for axis 1 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-d606cf30d7ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Optimization Finished\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Testing Accuracy:\"\u001b[0m\u001b[0;34m,\u001b[0m     \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmnist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 256 is out of bounds for axis 1 with size 10"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    while step * 128 < 20000:\n",
    "        batch_x, batch_y = mnist.train.next_batch(128)\n",
    "        sess.run(optimizer, feed_dict={x:batch_x,y:batch_y,keep_prob:dropout})\n",
    "        \n",
    "        if step % 10 == 0:\n",
    "            loss,acc = sess.run([cost,accuracy],feed_dict={x:batch_x,y:batch_y,keep_prob:1.})\n",
    "            print \"Iter \" + str(step*128) + \", minibatch loss=\" + \\\n",
    "            \"{:.6f}\".format(loss) + \", Training accuracy=\" + \\\n",
    "            \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished\"\n",
    "    \n",
    "    print \"Testing Accuracy:\", \\\n",
    "    sess.run(accuracy, feed_dict={x:mnist.test.images[:256],y:mnist.test.labels[:,256],keep_prob:1.})\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 784)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.test.images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
